---
title: "Anything2Vec: Mapping Reddit into Vector Spaces"
slug: "anything2vec"
emoji: "ðŸ’¥"
blurb: "Word2Vec is a powerful machine learning technique for embedding text corpus' into vector spaces. While useful for NLP problems, this blog post shows how it can also be used to represent and better understand communities on Reddit."
type: "bp"
tags: ["ml"]
link: "<a aria-label='Blog' href='https://medium.com/@cameronraymond/anything2vec-mapping-reddit-into-vector-spaces-dcc77d9f3bea'>Blog</a>"
date: "2020-07-02"
prod: true
---


A common problem in ML, natural language processing (NLP), and AI at large surrounds representing objects in a way computers can process. And since computers understand numbers â€”  which we have a common language for comparing, combining and manipulating â€” this generally means assigning objects numbers in some fashion. Think taking something abstract but intuitive to humans, like the text of a book, and assigning each word in that book a unique number. That book could then be represented by the list, or vector, of numbers assigned to it. This is the process *of embedding that book as a vector â€”* and there is an increasingly rich literature of techniques for embedding objects as vectors.