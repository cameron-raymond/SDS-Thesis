---
title: "Understanding Word2Vec through Cultural Dimensions"
slug: "word2vec-cultural-dims"
emoji: "ðŸ§«"
blurb: "Understanding the decisions AI make is critical in mitigating its downsides. This article explains what cultural dimensions are, and demonstrates how they can increase interpretability and quantify bias in word embeddings."
type: "bp"
tags: ["ml"]
link: "<a aria-label='Blog' href='https://medium.com/@cameronraymond/understanding-word2vec-through-cultural-dimensions-39934ae72926'>Blog</a>"
date: "2020-07-06"
prod: true
---
Word2Vec is a tool used to embed objects as vectors in a relationship preserving manner. It does so by following the intuition that words used in similar contexts likely have similar meanings. It was originally intended, as the name suggests, to embed words â€” but as my [previous article](https://cameronraymond.me/blog/anything2vec/) shows, it can be extended to represent any arbitrary set of objects that have a logical entity-context relation. And Iâ€™ll be the first to say that it is an incredibly clever and useful model.
